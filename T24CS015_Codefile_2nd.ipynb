{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "be0fd30ed3684c1182094f65215f9b9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fe287b079aa4b028d27361d81740e56",
              "IPY_MODEL_c84d1b36c9224a0f951570f9bed5eb5d",
              "IPY_MODEL_aefef68bdee841c1b3c42e907da37e7a"
            ],
            "layout": "IPY_MODEL_3e10cf197f754b23b6f4c24668c99925"
          }
        },
        "6fe287b079aa4b028d27361d81740e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ec9a41636f44a41a0e145284365a450",
            "placeholder": "​",
            "style": "IPY_MODEL_69833ff04fa74eaebf9fc9c2921acb4e",
            "value": "model.safetensors: 100%"
          }
        },
        "c84d1b36c9224a0f951570f9bed5eb5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb7aa42d4661434f942f0fc35a08eba2",
            "max": 86523256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72968c0abd5942049e889f931da11cdb",
            "value": 86523256
          }
        },
        "aefef68bdee841c1b3c42e907da37e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5f47b20d2f84c308a657629e0483f30",
            "placeholder": "​",
            "style": "IPY_MODEL_8f41aab06375459f9d316d2148cb0e44",
            "value": " 86.5M/86.5M [00:02&lt;00:00, 63.8MB/s]"
          }
        },
        "3e10cf197f754b23b6f4c24668c99925": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ec9a41636f44a41a0e145284365a450": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69833ff04fa74eaebf9fc9c2921acb4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb7aa42d4661434f942f0fc35a08eba2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72968c0abd5942049e889f931da11cdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5f47b20d2f84c308a657629e0483f30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f41aab06375459f9d316d2148cb0e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saucymukhim/Chisur-Academy-Test-01/blob/main/T24CS015_Codefile_2nd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EFFICIENTNETV2 + TRANSFORMER HEAD**"
      ],
      "metadata": {
        "id": "rLu1SWxe6oSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 1: SETUP & INSTALLATION"
      ],
      "metadata": {
        "id": "nUruxtt8-y25"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ulb3G8dQ-uPk"
      },
      "outputs": [],
      "source": [
        "# Import Google Drive library\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access files\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAMg9mwtA5Pv",
        "outputId": "632024a0-50be-4d0f-c5c6-477744e0c862"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/SaucyDataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y13fTrOtA673",
        "outputId": "6d36190e-fb76-4249-eba6-a687e63e937a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/SaucyDataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages for this project\n",
        "!pip install --upgrade -q timm scikit-learn seaborn\n",
        "!pip install optuna\n",
        "!pip install torchmetrics\n",
        "!pip install opencv-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lN-H4MIKA84e",
        "outputId": "34a7dc18-ae6d-43da-a81d-ff20569b6009"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/8.9 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/8.9 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optuna\n",
            "  Downloading optuna-4.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.18.2)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.7.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.9/413.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.7.0\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.13.0.90)\n",
            "Requirement already satisfied: numpy>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "!pip install opencv-python\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGDExt80tlaL",
        "outputId": "9713b42d-0862-45d4-f08d-3f7dfd670964"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.13.0.90)\n",
            "Requirement already satisfied: numpy>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for deep learning, data manipulation, and visualization.\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import timm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                            accuracy_score, roc_auc_score, roc_curve,\n",
        "                            precision_recall_curve, average_precision_score,\n",
        "                            precision_recall_fscore_support)\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import random\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For hyperparameter tuning\n",
        "import optuna\n",
        "\n",
        "# For model calibration\n",
        "from torchmetrics import CalibrationError\n",
        "\n",
        "print(\"✅ All packages installed and imported successfully!\")"
      ],
      "metadata": {
        "id": "yzh6nIR6A_YW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59012b8d-66dd-4634-c36d-329e3a509a48"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All packages installed and imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 2: CONFIGURATION"
      ],
      "metadata": {
        "id": "RZPPjRR8BFto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Central configuration class to manage all hyperparameters, paths, and settings.\n",
        "class Config:\n",
        "    \"\"\"Configuration class for the bone fracture detection project.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Data paths\n",
        "        self.DATA_PATH = \"/content/drive/MyDrive/SaucyDataset\"\n",
        "        self.OUTPUT_DIR = \"/content/drive/MyDrive/SaucyDataset_Output\"\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "        # Model & Training Parameters\n",
        "        self.MODEL_NAME = 'tf_efficientnetv2_s.in1k'\n",
        "        self.IMAGE_SIZE = 256\n",
        "        self.BATCH_SIZE = 8\n",
        "        self.EPOCHS = 30\n",
        "        self.LEARNING_RATE = 1e-4\n",
        "        self.NUM_CLASSES = 2  # 'fractured' and 'not_fractured'\n",
        "\n",
        "        # Transformer Head Parameters\n",
        "        self.D_MODEL = 256\n",
        "        self.NHEAD = 8\n",
        "        self.NUM_ENCODER_LAYERS = 4\n",
        "        self.DIM_FEEDFORWARD = 2048\n",
        "        self.DROPOUT = 0.1\n",
        "\n",
        "        # Early stopping parameters\n",
        "        self.EARLY_STOPPING_PATIENCE = 5\n",
        "\n",
        "        # K-Fold CV parameters\n",
        "        self.K_FOLDS = 2\n",
        "\n",
        "        # For reproducibility\n",
        "        self.SEED = 42\n",
        "\n",
        "        # Device configuration\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create a global configuration instance to be used throughout the script\n",
        "CFG = Config()\n",
        "\n",
        "print(f\"✅ Configuration loaded. Device set to: {CFG.device}\")\n"
      ],
      "metadata": {
        "id": "c6azka_2BKSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f344b0c-ccd1-4d47-84bb-9fe1327cfe31"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Configuration loaded. Device set to: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 3: DATA HANDLING & TRANSFORMATIONS"
      ],
      "metadata": {
        "id": "miHtK42fBbzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classes responsible for loading data, applying transformations, and managing datasets.\n",
        "class DataTransforms:\n",
        "    \"\"\"Class to handle data transformations for different phases (train, val, test, tta).\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_transforms(image_size, phase='train'):\n",
        "        \"\"\"Get data transforms based on phase.\"\"\"\n",
        "        if phase == 'train':\n",
        "            return transforms.Compose([\n",
        "                transforms.RandomResizedCrop(image_size),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation(10),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        elif phase in ['val', 'test']:\n",
        "            return transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(image_size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        elif phase == 'tta':  # Test Time Augmentation\n",
        "            return [\n",
        "                transforms.Compose([transforms.Resize(256), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "                transforms.Compose([transforms.Resize(256), transforms.CenterCrop(image_size), transforms.RandomHorizontalFlip(p=1.0), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "                transforms.Compose([transforms.Resize(256), transforms.CenterCrop(image_size), transforms.RandomRotation(10), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "            ]\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown phase: {phase}\")\n",
        "\n",
        "class DatasetManager:\n",
        "    \"\"\"Class to handle dataset operations like loading, splitting, and creating dataloaders.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_dataset(data_path):\n",
        "        \"\"\"Load X-ray dataset from folders.\"\"\"\n",
        "        if not os.path.exists(data_path):\n",
        "            raise FileNotFoundError(f\"Data path not found: {data_path}. Please check your CFG.DATA_PATH.\")\n",
        "        print(f\"Loading dataset from {data_path}\")\n",
        "        return data_path\n",
        "\n",
        "    @staticmethod\n",
        "    def create_dataset_splits(data_path, image_size):\n",
        "        \"\"\"Create dataset partitions for train, validation, and test.\"\"\"\n",
        "        data_transforms = {\n",
        "            'train': DataTransforms.get_transforms(image_size, 'train'),\n",
        "            'val': DataTransforms.get_transforms(image_size, 'val'),\n",
        "            'test': DataTransforms.get_transforms(image_size, 'test')\n",
        "        }\n",
        "        image_datasets = {x: datasets.ImageFolder(os.path.join(data_path, x), data_transforms[x])\n",
        "                         for x in ['train', 'val', 'test']}\n",
        "        return image_datasets\n",
        "\n",
        "    @staticmethod\n",
        "    def create_dataloaders(image_datasets, batch_size):\n",
        "        \"\"\"Create batched, shuffled data loaders.\"\"\"\n",
        "        dataloaders = {x: DataLoader(image_datasets[x], batch_size=batch_size,\n",
        "                                    shuffle=(x=='train'), num_workers=2)\n",
        "                      for x in ['train', 'val', 'test']}\n",
        "        dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
        "        class_names = image_datasets['train'].classes\n",
        "\n",
        "        print(f\"Datasets loaded. Class names: {class_names}\")\n",
        "        for split, size in dataset_sizes.items():\n",
        "            print(f\"   - {split.capitalize()} set size: {size} images\")\n",
        "        return dataloaders, dataset_sizes, class_names\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_metadata(image_datasets):\n",
        "        \"\"\"Prepare metadata including file paths and labels for multi-center validation.\"\"\"\n",
        "        metadata = {}\n",
        "        for split in ['train', 'val', 'test']:\n",
        "            dataset = image_datasets[split]\n",
        "            samples = dataset.samples\n",
        "            class_to_idx = dataset.class_to_idx\n",
        "            split_metadata = []\n",
        "            for path, label in samples:\n",
        "                filename = os.path.basename(path)\n",
        "                class_name = [k for k, v in class_to_idx.items() if v == label][0]\n",
        "                site_id = f\"site_{random.randint(1, 5)}\" # Simulated site ID\n",
        "                split_metadata.append({'filepath': path, 'filename': filename, 'label': label, 'class_name': class_name, 'site_id': site_id})\n",
        "            metadata[split] = split_metadata\n",
        "        with open(os.path.join(CFG.OUTPUT_DIR, 'metadata.json'), 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        print(f\"Metadata prepared and saved to {os.path.join(CFG.OUTPUT_DIR, 'metadata.json')}\")\n",
        "        return metadata\n",
        "\n",
        "print(\"✅ Data Handling classes defined.\")"
      ],
      "metadata": {
        "id": "_4xFd_ejBdYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "400f8182-2408-4931-bfa5-7560a67e57c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data Handling classes defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 4: MODEL ARCHITECTURE"
      ],
      "metadata": {
        "id": "fVry0jwfBoU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of the Hybrid CNN-Transformer model.\n",
        "class HybridModel(nn.Module):\n",
        "    \"\"\"EfficientNetV2 backbone + Transformer head.\"\"\"\n",
        "\n",
        "    def __init__(self, cnn_model_name, num_classes, d_model, nhead, num_encoder_layers, dim_feedforward, dropout):\n",
        "        super(HybridModel, self).__init__()\n",
        "\n",
        "        # 1. CNN Backbone (EfficientNetV2)\n",
        "        self.backbone = timm.create_model(cnn_model_name, pretrained=True, features_only=True, out_indices=[-1])\n",
        "\n",
        "        # 2. Transformer Head\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        # 3. Classification Head\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Define the forward pass of the model.\"\"\"\n",
        "        features = self.backbone(x)[0]  # Shape: (batch_size, d_model, H, W)\n",
        "        batch_size = features.shape[0]\n",
        "        seq = features.flatten(2).permute(0, 2, 1)  # (batch_size, seq_len, d_model)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        seq = torch.cat((cls_tokens, seq), dim=1)\n",
        "        transformer_output = self.transformer_encoder(seq)\n",
        "        cls_output = transformer_output[:, 0, :]\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits\n",
        "\n",
        "print(\"✅ HybridModel architecture defined.\")"
      ],
      "metadata": {
        "id": "KtwJQ073BpPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2687d83-5485-4309-bda5-6eb636c2c780"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ HybridModel architecture defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 5: MODEL UTILITIES"
      ],
      "metadata": {
        "id": "wBS8uOBQBrG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions for model operations like freezing layers, setting up optimizers, etc.\n",
        "class ModelUtils:\n",
        "    \"\"\"Utility functions for model operations.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def freeze_backbone(model):\n",
        "        \"\"\"Freeze backbone parameters.\"\"\"\n",
        "        for param in model.backbone.parameters(): param.requires_grad = False\n",
        "        print(\"Backbone frozen\")\n",
        "\n",
        "    @staticmethod\n",
        "    def unfreeze_backbone(model):\n",
        "        \"\"\"Unfreeze backbone parameters.\"\"\"\n",
        "        for param in model.backbone.parameters(): param.requires_grad = True\n",
        "        print(\"Backbone unfrozen\")\n",
        "\n",
        "    @staticmethod\n",
        "    def initialize_weights(model):\n",
        "        \"\"\"Apply model initialization.\"\"\"\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight); nn.init.constant_(m.bias, 0) if m.bias is not None else None\n",
        "            elif isinstance(m, nn.LayerNorm): nn.init.constant_(m.bias, 0); nn.init.constant_(m.weight, 1.0)\n",
        "        print(\"Weights initialized\")\n",
        "\n",
        "    @staticmethod\n",
        "    def get_loss_function(loss_type='cross_entropy'):\n",
        "        \"\"\"Define Cross-entropy / focal loss.\"\"\"\n",
        "        return nn.CrossEntropyLoss()\n",
        "\n",
        "    @staticmethod\n",
        "    def get_optimizer(model, learning_rate, weight_decay=1e-5):\n",
        "        \"\"\"AdamW optimizer settings.\"\"\"\n",
        "        return optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_lr_scheduler(optimizer, scheduler_type='cosine', epochs=8):\n",
        "        \"\"\"Cosine annealing or ReduceLROnPlateau.\"\"\"\n",
        "        if scheduler_type == 'cosine': return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
        "        elif scheduler_type == 'reduce_on_plateau': return optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
        "        else: return optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "    @staticmethod\n",
        "    def save_checkpoint(model, optimizer, epoch, val_acc, path):\n",
        "        \"\"\"Save best-performing model.\"\"\"\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'val_acc': val_acc}, path)\n",
        "        print(f\"Checkpoint saved to {path}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def load_best_model(model, checkpoint_path):\n",
        "        \"\"\"Load trained checkpoint.\"\"\"\n",
        "        checkpoint = torch.load(checkpoint_path); model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(f\"Model loaded from {checkpoint_path}\"); return model\n",
        "\n",
        "print(\"✅ Model utility functions defined.\")"
      ],
      "metadata": {
        "id": "ojb0u3_TBsL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad841f1-69aa-46de-99f0-095c3e0b33ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model utility functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 6: TRAINING & EVALUATION LOGIC"
      ],
      "metadata": {
        "id": "JropSryyBuTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingUtils:\n",
        "    \"\"\"Utility functions for training and evaluation.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "        \"\"\"Train model for a single epoch.\"\"\"\n",
        "        model.train()\n",
        "        running_loss, correct_predictions, total_samples = 0.0, 0, 0\n",
        "        progress_bar = tqdm(dataloader, desc=\"Training\", unit=\"batch\")\n",
        "\n",
        "        for inputs, labels in progress_bar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_predictions += torch.sum(preds == labels.data)\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            progress_bar.set_postfix(\n",
        "                loss=loss.item(),\n",
        "                acc=f\"{(correct_predictions.double() / total_samples):.4f}\"\n",
        "            )\n",
        "\n",
        "        epoch_loss = running_loss / total_samples\n",
        "        epoch_acc = correct_predictions.double() / total_samples\n",
        "        return epoch_loss, epoch_acc.item()\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate(model, dataloader, criterion, device, split='Validation'):\n",
        "        \"\"\"Evaluate model for one epoch on a given split.\"\"\"\n",
        "        model.eval()\n",
        "        running_loss, all_preds, all_labels, all_probs = 0.0, [], [], []\n",
        "        progress_bar = tqdm(dataloader, desc=split, unit=\"batch\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in progress_bar:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "        total_samples = len(all_labels)\n",
        "        epoch_loss = running_loss / total_samples\n",
        "        epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        return epoch_loss, epoch_acc, all_labels, all_preds, all_probs\n",
        "\n",
        "    @staticmethod\n",
        "    def test_inference(model, test_loader, criterion, device):\n",
        "        \"\"\"Run model on the test set and report metrics.\"\"\"\n",
        "        model.eval()\n",
        "        test_loss, test_acc, test_labels, test_preds, test_probs = TrainingUtils.evaluate(\n",
        "            model, test_loader, criterion, device, split='Test'\n",
        "        )\n",
        "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "        return test_loss, test_acc, test_labels, test_preds, test_probs\n",
        "\n",
        "\n",
        "print(\"✅ Training and evaluation logic defined.\")\n"
      ],
      "metadata": {
        "id": "vYZ7_TAiBvPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a988c2a4-8759-4e6d-8900-e87c2bc4854a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training and evaluation logic defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 7: METRICS & VISUALIZATION"
      ],
      "metadata": {
        "id": "xxe6UvWdBxnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsAndVisualization:\n",
        "    \"\"\"Class for computing metrics and creating visualizations.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_metrics(labels, preds, probs):\n",
        "        \"\"\"Compute Accuracy, Precision, Recall, F1, and AUC.\"\"\"\n",
        "        # No artificial accuracy cap or label flipping\n",
        "        accuracy = accuracy_score(labels, preds)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            labels, preds, average='binary'\n",
        "        )\n",
        "        auc = roc_auc_score(labels, probs)\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'auc': auc\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_curves(history, test_acc=None, test_auc=None):\n",
        "        \"\"\"Plot training/validation accuracy and loss curves.\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
        "\n",
        "        ax1.plot(history['train_acc'], label='Train Accuracy', color='#1f77b4',\n",
        "                 marker='o', linestyle='-', lw=2)\n",
        "        ax1.plot(history['val_acc'], label='Validation Accuracy', color='#ff7f0e',\n",
        "                 marker='o', linestyle='-', lw=2)\n",
        "        if test_acc is not None:\n",
        "            ax1.axhline(\n",
        "                y=test_acc,\n",
        "                color='#2ca02c',\n",
        "                linestyle='--',\n",
        "                label=f'Test Accuracy: {test_acc:.4f}',\n",
        "                lw=2\n",
        "            )\n",
        "        ax1.set_title('Model Accuracy Over Epochs', fontsize=16, fontweight='bold')\n",
        "        ax1.set_xlabel('Epoch', fontsize=12)\n",
        "        ax1.set_ylabel('Accuracy', fontsize=12)\n",
        "        ax1.legend(fontsize=10)\n",
        "        ax1.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "        ax2.plot(history['train_loss'], label='Train Loss', color='#1f77b4',\n",
        "                 marker='o', linestyle='-', lw=2)\n",
        "        ax2.plot(history['val_loss'], label='Validation Loss', color='#ff7f0e',\n",
        "                 marker='o', linestyle='-', lw=2)\n",
        "        ax2.set_title('Model Loss Over Epochs', fontsize=16, fontweight='bold')\n",
        "        ax2.set_xlabel('Epoch', fontsize=12)\n",
        "        ax2.set_ylabel('Loss', fontsize=12)\n",
        "        ax2.legend(fontsize=10)\n",
        "        ax2.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(CFG.OUTPUT_DIR, 'training_curves.png'))\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_confusion_matrix(labels, preds, class_names):\n",
        "        \"\"\"Visualize confusion matrix.\"\"\"\n",
        "        cm = confusion_matrix(labels, preds)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(\n",
        "            cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names\n",
        "        )\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.title('Confusion Matrix for Test Set')\n",
        "        plt.savefig(os.path.join(CFG.OUTPUT_DIR, 'confusion_matrix.png'))\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def analyze_predictions(model, dataloader, class_names, device, num_examples=5):\n",
        "        \"\"\"Visualize correct & misclassified image examples.\"\"\"\n",
        "        model.eval()\n",
        "        all_preds, all_labels, all_probs, all_images = [], [], [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "                all_images.extend(inputs.cpu())\n",
        "\n",
        "        all_preds = np.array(all_preds)\n",
        "        all_labels = np.array(all_labels)\n",
        "        all_probs = np.array(all_probs)\n",
        "\n",
        "        # No accuracy cap or flipping here either\n",
        "        correct_indices = np.where(all_preds == all_labels)[0]\n",
        "        incorrect_indices = np.where(all_preds != all_labels)[0]\n",
        "\n",
        "        print(f\"\\nDisplaying {min(num_examples, len(correct_indices))} correct predictions\")\n",
        "        for i, idx in enumerate(\n",
        "            np.random.choice(correct_indices, min(num_examples, len(correct_indices)), replace=False)\n",
        "        ):\n",
        "            img = all_images[idx].permute(1, 2, 0).numpy()\n",
        "            img = np.clip(\n",
        "                img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]),\n",
        "                0, 1\n",
        "            )\n",
        "            true_label = class_names[all_labels[idx]]\n",
        "            pred_label = class_names[all_preds[idx]]\n",
        "            confidence = all_probs[idx, all_preds[idx]]\n",
        "\n",
        "            plt.figure(figsize=(4, 4))\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"True: {true_label}\\nPred: {pred_label} ({confidence:.2f})\",\n",
        "                      color='green')\n",
        "            plt.axis('off')\n",
        "            plt.savefig(os.path.join(CFG.OUTPUT_DIR, f'correct_pred_{i}.png'))\n",
        "            plt.show()\n",
        "\n",
        "        print(f\"\\nDisplaying {min(num_examples, len(incorrect_indices))} incorrect predictions\")\n",
        "        for i, idx in enumerate(\n",
        "            np.random.choice(incorrect_indices, min(num_examples, len(incorrect_indices)), replace=False)\n",
        "        ):\n",
        "            img = all_images[idx].permute(1, 2, 0).numpy()\n",
        "            img = np.clip(\n",
        "                img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]),\n",
        "                0, 1\n",
        "            )\n",
        "            true_label = class_names[all_labels[idx]]\n",
        "            pred_label = class_names[all_preds[idx]]\n",
        "            confidence = all_probs[idx, all_preds[idx]]\n",
        "\n",
        "            plt.figure(figsize=(4, 4))\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"True: {true_label}\\nPred: {pred_label} ({confidence:.2f})\",\n",
        "                      color='red')\n",
        "            plt.axis('off')\n",
        "            plt.savefig(os.path.join(CFG.OUTPUT_DIR, f'incorrect_pred_{i}.png'))\n",
        "            plt.show()\n",
        "\n",
        "print(\"✅ Metrics and visualization functions defined.\")\n"
      ],
      "metadata": {
        "id": "GjrJ08xZBytF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "558a3aa8-04de-4c0f-a0e1-b6b5e4432362"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Metrics and visualization functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 8: GRAD-CAM IMPLEMENTATION"
      ],
      "metadata": {
        "id": "0oA1LXxnB1Gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradCAM:\n",
        "    \"\"\"\n",
        "    Implements Gradient-weighted Class Activation Mapping (Grad-CAM).\n",
        "\n",
        "    This class hooks into the forward and backward pass of the model's\n",
        "    last convolutional layer to calculate feature map importance weights.\n",
        "    \"\"\"\n",
        "    def __init__(self, model: torch.nn.Module, target_layer_name: str):\n",
        "        self.model = model\n",
        "        self.target_layer = self._find_target_layer(target_layer_name)\n",
        "        self.feature_maps = {}\n",
        "        self.gradients = {}\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _find_target_layer(self, name: str):\n",
        "        # Find the target layer in the EfficientNetV2 backbone\n",
        "        modules = dict(self.model.backbone.named_modules())\n",
        "\n",
        "        # First, try to use the provided name directly if it exists and is a Conv2d\n",
        "        if name in modules and isinstance(modules[name], nn.Conv2d):\n",
        "            print(f\"Using specified target layer: '{name}'\")\n",
        "            return modules[name]\n",
        "\n",
        "        # If the specified name is not found or not a Conv2d,\n",
        "        # iterate to find the last Conv2d layer\n",
        "        last_conv_layer = None\n",
        "        last_conv_name = None\n",
        "        for n, m in modules.items():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                last_conv_layer = m\n",
        "                last_conv_name = n\n",
        "\n",
        "        if last_conv_layer is not None:\n",
        "            print(f\"Specified target layer '{name}' not found or not a Conv2d. Using last found Conv2d layer: '{last_conv_name}' as target.\")\n",
        "            return last_conv_layer\n",
        "        else:\n",
        "            raise ValueError(f\"Could not find any Conv2d layer in the model's backbone for Grad-CAM. Named modules: {modules.keys()}\")\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        \"\"\"Registers hooks to capture feature maps (forward) and gradients (backward).\"\"\"\n",
        "        def forward_hook(module, input, output):\n",
        "            # Store the feature map (output of the layer)\n",
        "            self.feature_maps[0] = output.detach()\n",
        "\n",
        "        def backward_hook(module, grad_in, grad_out):\n",
        "            # Store the gradient (input to the layer, which is grad_out)\n",
        "            self.gradients[0] = grad_out[0].detach()\n",
        "\n",
        "        # Register the hooks on the identified target layer\n",
        "        self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def __call__(self, input_tensor: torch.Tensor, target_category: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes the Grad-CAM heatmap for a single input tensor.\n",
        "\n",
        "        Args:\n",
        "            input_tensor: The image tensor (e.g., [1, 3, 224, 224]).\n",
        "            target_category: The index of the class to visualize (e.g., 0 for 'normal', 1 for 'fractured').\n",
        "\n",
        "        Returns:\n",
        "            A normalized and resized NumPy array representing the heatmap.\n",
        "        \"\"\"\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        # 1. Forward pass\n",
        "        output = self.model(input_tensor)\n",
        "\n",
        "        # 2. Select the target score and perform a backward pass\n",
        "        # The output must be a single item tensor for backward to work\n",
        "        target_score = output[:, target_category]\n",
        "        target_score.backward(retain_graph=True)\n",
        "\n",
        "        # 3. Get the stored feature maps and gradients\n",
        "        feature_maps = self.feature_maps[0]  # [B, C, H, W]\n",
        "        gradients = self.gradients[0]      # [B, C, H, W]\n",
        "\n",
        "        # We only process the first item in the batch (B=1)\n",
        "        # 4. Calculate Alpha (Neuron Importance Weights)\n",
        "        # Global Average Pooling (GAP) of the gradients across spatial dimensions (H, W)\n",
        "        weights = torch.mean(gradients, dim=(2, 3), keepdim=True) # [1, C, 1, 1]\n",
        "\n",
        "        # 5. Compute the weighted feature map\n",
        "        # Sum of (alpha * feature_map) across the channel dimension (C)\n",
        "        weighted_feature_map = weights * feature_maps\n",
        "        cam = torch.sum(weighted_feature_map, dim=1) # [1, H, W]\n",
        "\n",
        "        # 6. Apply ReLU to the CAM\n",
        "        heatmap = F.relu(cam)\n",
        "\n",
        "        # 7. Normalize and resize\n",
        "        heatmap = heatmap.squeeze(0).cpu().numpy()\n",
        "        heatmap = heatmap / np.max(heatmap) # Normalize to 0-1\n",
        "\n",
        "        # Resize the heatmap to the original image size (224x224 from CFG)\n",
        "        heatmap = cv2.resize(heatmap, (input_tensor.shape[2], input_tensor.shape[3]))\n",
        "\n",
        "        return heatmap"
      ],
      "metadata": {
        "id": "1I2ojFo-B2Mf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 9: GRAD-CAM VISUALIZATION UTILITIES"
      ],
      "metadata": {
        "id": "wfN4WIyoB4wF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions for visualizing Grad-CAM heatmaps.\n",
        "def visualize_grad_cam(original_image: np.ndarray, heatmap: np.ndarray, title: str, save_path=None):\n",
        "    \"\"\"\n",
        "    Overlays the heatmap on the original image for visualization.\n",
        "\n",
        "    Args:\n",
        "        original_image: The original image as a NumPy array (H, W, 3).\n",
        "        heatmap: The Grad-CAM heatmap (H, W) normalized between 0 and 1.\n",
        "        title: The title for the plot.\n",
        "        save_path: Optional path to save the visualization.\n",
        "    \"\"\"\n",
        "    # Convert image to 0-255 range and 8-bit unsigned integer (for OpenCV color mapping)\n",
        "    image_float = (original_image * 255).astype(np.uint8)\n",
        "\n",
        "    # Apply colormap (e.g., JET)\n",
        "    heatmap_jet = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
        "    heatmap_jet = cv2.cvtColor(heatmap_jet, cv2.COLOR_BGR2RGB) # Convert BGR to RGB\n",
        "\n",
        "    # Superimpose the heatmap on the image\n",
        "    # Alpha (0.4) controls the transparency of the heatmap\n",
        "    superimposed_img = cv2.addWeighted(image_float, 0.6, heatmap_jet, 0.4, 0)\n",
        "\n",
        "    # Plotting\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    ax[0].imshow(original_image)\n",
        "    ax[0].set_title(f\"Original: {title}\", fontsize=10)\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    ax[1].imshow(superimposed_img)\n",
        "    ax[1].set_title(f\"Grad-CAM: {title}\", fontsize=10)\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "    plt.show()\n",
        "\n",
        "class GradCAMUtils:\n",
        "    \"\"\"Utility class for Grad-CAM operations.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_and_save_gradcam_samples(model, dataloader, class_names, device, num_samples=5):\n",
        "        \"\"\"\n",
        "        Generate Grad-CAM visualizations for random samples from the dataset.\n",
        "\n",
        "        Args:\n",
        "            model: Trained model for visualization\n",
        "            dataloader: DataLoader containing the images\n",
        "            class_names: List of class names\n",
        "            device: Device to run computations on\n",
        "            num_samples: Number of samples to visualize\n",
        "        \"\"\"\n",
        "        print(f\"\\nGenerating {num_samples} Grad-CAM visualizations...\")\n",
        "\n",
        "        # Initialize GradCAM with the target layer\n",
        "        # For EfficientNetV2, we'll use the last feature block\n",
        "        grad_cam = GradCAM(model, target_layer_name='features.8')\n",
        "\n",
        "        # Select random samples from the dataset\n",
        "        sample_indices = np.random.choice(len(dataloader.dataset), num_samples, replace=False)\n",
        "\n",
        "        for i, idx in enumerate(sample_indices):\n",
        "            # Get the image and label\n",
        "            image, label = dataloader.dataset[idx]\n",
        "            image_tensor = image.unsqueeze(0).to(device)\n",
        "\n",
        "            # Get model prediction\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                logits = model(image_tensor)\n",
        "                pred_prob = F.softmax(logits, dim=1)\n",
        "                pred_label = torch.argmax(pred_prob).item()\n",
        "\n",
        "            # Compute Grad-CAM for the predicted class\n",
        "            heatmap = grad_cam(image_tensor, target_category=pred_label)\n",
        "\n",
        "            # Prepare the image for visualization (convert from C, H, W to H, W, C)\n",
        "            image_np = image_tensor.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "            # Denormalize the image\n",
        "            mean = np.array([0.485, 0.456, 0.406])\n",
        "            std = np.array([0.229, 0.224, 0.225])\n",
        "            image_np = image_np * std + mean\n",
        "            image_np = np.clip(image_np, 0, 1)\n",
        "\n",
        "            # Create title\n",
        "            title = (f\"Pred: {class_names[pred_label]} ({pred_prob[0, pred_label].item():.2f}) \"\n",
        "                     f\"| True: {class_names[label]}\")\n",
        "\n",
        "            # Visualize the Grad-CAM\n",
        "            save_path = os.path.join(CFG.OUTPUT_DIR, f'gradcam_sample_{i}.png')\n",
        "            visualize_grad_cam(image_np, heatmap, title, save_path)\n",
        "\n",
        "        print(f\"✅ Grad-CAM visualizations saved to {CFG.OUTPUT_DIR}\")\n",
        "\n",
        "print(\"✅ Grad-CAM visualization utilities defined.\")"
      ],
      "metadata": {
        "id": "fE253qxHB6Em",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "637d5683-e3a6-4471-892d-8413e6d425f5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Grad-CAM visualization utilities defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 10: ADVANCED TECHNIQUES"
      ],
      "metadata": {
        "id": "M-F1Ewu-B8c1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedTechniques:\n",
        "    \"\"\"Class for implementing advanced techniques.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def k_fold_cross_validation(data_path, image_size, batch_size, k_folds=2):\n",
        "        \"\"\"Implement stratified K-fold cross-validation.\"\"\"\n",
        "        print(f\"Starting {k_folds}-Fold Cross Validation\")\n",
        "        full_dataset = datasets.ImageFolder(\n",
        "            os.path.join(data_path, 'train'),\n",
        "            transform=DataTransforms.get_transforms(image_size, 'train')\n",
        "        )\n",
        "        labels = [label for _, label in full_dataset.samples]\n",
        "        skf = StratifiedKFold(\n",
        "            n_splits=k_folds,\n",
        "            shuffle=True,\n",
        "            random_state=CFG.SEED\n",
        "        )\n",
        "        fold_results = []\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
        "            print(f\"\\nFold {fold+1}/{k_folds}\")\n",
        "            train_subset = Subset(full_dataset, train_idx)\n",
        "            val_subset = Subset(full_dataset, val_idx)\n",
        "\n",
        "            train_loader = DataLoader(\n",
        "                train_subset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                num_workers=2\n",
        "            )\n",
        "            val_loader = DataLoader(\n",
        "                val_subset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=2\n",
        "            )\n",
        "\n",
        "            model = HybridModel(\n",
        "                cnn_model_name=CFG.MODEL_NAME,\n",
        "                num_classes=CFG.NUM_CLASSES,\n",
        "                d_model=CFG.D_MODEL,\n",
        "                nhead=CFG.NHEAD,\n",
        "                num_encoder_layers=CFG.NUM_ENCODER_LAYERS,\n",
        "                dim_feedforward=CFG.DIM_FEEDFORWARD,\n",
        "                dropout=CFG.DROPOUT\n",
        "            ).to(CFG.device)\n",
        "\n",
        "            ModelUtils.initialize_weights(model)\n",
        "            criterion = ModelUtils.get_loss_function()\n",
        "            optimizer = ModelUtils.get_optimizer(model, CFG.LEARNING_RATE)\n",
        "            scheduler = ModelUtils.get_lr_scheduler(optimizer, epochs=CFG.EPOCHS)\n",
        "\n",
        "            best_val_acc = 0.0\n",
        "            for epoch in range(CFG.EPOCHS):\n",
        "                train_loss, train_acc = TrainingUtils.train_one_epoch(\n",
        "                    model, train_loader, criterion, optimizer, CFG.device\n",
        "                )\n",
        "                val_loss, val_acc, _, _, _ = TrainingUtils.evaluate(\n",
        "                    model, val_loader, criterion, CFG.device, split='Validation'\n",
        "                )\n",
        "                scheduler.step()\n",
        "\n",
        "                if val_acc > best_val_acc:\n",
        "                    best_val_acc = val_acc\n",
        "                    torch.save(\n",
        "                        model.state_dict(),\n",
        "                        os.path.join(CFG.OUTPUT_DIR, f'fold_{fold+1}_best_model.pth')\n",
        "                    )\n",
        "\n",
        "            fold_results.append({'fold': fold+1, 'best_val_acc': best_val_acc})\n",
        "            print(f\"Fold {fold+1} completed. Best validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "        avg_val_acc = np.mean([result['best_val_acc'] for result in fold_results])\n",
        "        std_val_acc = np.std([result['best_val_acc'] for result in fold_results])\n",
        "        print(f\"\\nCross-Validation Results\\nAverage validation accuracy: {avg_val_acc:.4f} ± {std_val_acc:.4f}\")\n",
        "\n",
        "        results_df = pd.DataFrame(fold_results)\n",
        "        results_df.to_csv(os.path.join(CFG.OUTPUT_DIR, 'kfold_results.csv'), index=False)\n",
        "\n",
        "        return fold_results\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_kfold_results(fold_results):\n",
        "        \"\"\"Visualizes K-Fold Cross-Validation results with a bar chart.\"\"\"\n",
        "        print(\"\\nGenerating K-Fold visualization\")\n",
        "        folds = [f\"Fold {result['fold']}\" for result in fold_results]\n",
        "        accuracies = [result['best_val_acc'] for result in fold_results]\n",
        "\n",
        "        avg_acc = np.mean(accuracies)\n",
        "        std_acc = np.std(accuracies)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        bars = plt.bar(folds, accuracies, color='skyblue', alpha=0.7, yerr=std_acc, capsize=5)\n",
        "        plt.axhline(\n",
        "            y=avg_acc,\n",
        "            color='red',\n",
        "            linestyle='--',\n",
        "            label=f'Average Accuracy: {avg_acc:.4f} ± {std_acc:.4f}'\n",
        "        )\n",
        "\n",
        "        for bar, acc in zip(bars, accuracies):\n",
        "            plt.text(\n",
        "                bar.get_x() + bar.get_width()/2,\n",
        "                bar.get_height() + std_acc + 0.002,\n",
        "                f'{acc:.4f}',\n",
        "                ha='center',\n",
        "                va='bottom',\n",
        "                fontsize=10\n",
        "            )\n",
        "\n",
        "        plt.xlabel('Cross-Validation Fold', fontsize=12)\n",
        "        plt.ylabel('Best Validation Accuracy', fontsize=12)\n",
        "        plt.title('K-Fold Cross-Validation Performance', fontsize=16, fontweight='bold')\n",
        "        plt.ylim(0, 1)\n",
        "        plt.legend(fontsize=11)\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(CFG.OUTPUT_DIR, 'kfold_results_plot.png'))\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def objective(trial, train_loader, val_loader):\n",
        "        \"\"\"Objective function for Optuna hyperparameter tuning.\"\"\"\n",
        "        lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
        "        weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
        "        dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
        "        nhead = trial.suggest_categorical('nhead', [4, 8, 16])\n",
        "\n",
        "        model = HybridModel(\n",
        "            cnn_model_name=CFG.MODEL_NAME,\n",
        "            num_classes=CFG.NUM_CLASSES,\n",
        "            d_model=CFG.D_MODEL,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=CFG.NUM_ENCODER_LAYERS,\n",
        "            dim_feedforward=CFG.DIM_FEEDFORWARD,\n",
        "            dropout=dropout\n",
        "        ).to(CFG.device)\n",
        "\n",
        "        criterion = ModelUtils.get_loss_function()\n",
        "        optimizer = ModelUtils.get_optimizer(model, lr, weight_decay)\n",
        "\n",
        "        best_val_acc = 0.0\n",
        "        for epoch in range(2):\n",
        "            train_loss, train_acc = TrainingUtils.train_one_epoch(\n",
        "                model, train_loader, criterion, optimizer, CFG.device\n",
        "            )\n",
        "            val_loss, val_acc, _, _, _ = TrainingUtils.evaluate(\n",
        "                model, val_loader, criterion, CFG.device, split='Validation'\n",
        "            )\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "\n",
        "        return best_val_acc\n",
        "\n",
        "    @staticmethod\n",
        "    def hyperparameter_tuning(train_loader, val_loader, n_trials=2):\n",
        "        \"\"\"Perform hyperparameter tuning using Optuna.\"\"\"\n",
        "        print(f\"Starting hyperparameter tuning with {n_trials} trials\")\n",
        "        study = optuna.create_study(direction='maximize')\n",
        "        study.optimize(\n",
        "            lambda trial: AdvancedTechniques.objective(trial, train_loader, val_loader),\n",
        "            n_trials=n_trials\n",
        "        )\n",
        "\n",
        "        best_params = study.best_params\n",
        "        best_value = study.best_value\n",
        "\n",
        "        print(f\"Best parameters: {best_params}\\nBest validation accuracy: {best_value:.4f}\")\n",
        "\n",
        "        with open(os.path.join(CFG.OUTPUT_DIR, 'best_hyperparams.json'), 'w') as f:\n",
        "            json.dump(best_params, f, indent=2)\n",
        "\n",
        "        return best_params\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate_model_calibration(model, dataloader, device):\n",
        "        \"\"\"Compute ECE and plot a reliability diagram.\"\"\"\n",
        "        model.eval()\n",
        "        calibrator = CalibrationError(\n",
        "            task=\"multiclass\",\n",
        "            num_classes=CFG.NUM_CLASSES\n",
        "        ).to(device)\n",
        "\n",
        "        all_preds, all_labels, all_probs = [], [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "        all_preds = torch.tensor(all_preds).to(device)\n",
        "        all_labels = torch.tensor(all_labels).to(device)\n",
        "        all_probs = torch.tensor(all_probs).to(device)\n",
        "\n",
        "        # No accuracy cap or label flipping here\n",
        "        ece = calibrator(all_probs, all_labels)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.hist2d(\n",
        "            all_probs[:, 1].cpu().numpy(),\n",
        "            (all_preds == 1).cpu().numpy(),\n",
        "            bins=10,\n",
        "            range=[[0, 1], [0, 1]],\n",
        "            cmap='Blues'\n",
        "        )\n",
        "        plt.plot([0, 1], [0, 1], 'r--')\n",
        "        plt.xlabel('Mean predicted probability')\n",
        "        plt.ylabel('Fraction of positives')\n",
        "        plt.title(f'Reliability Diagram (ECE: {ece.item():.4f})')\n",
        "        plt.savefig(os.path.join(CFG.OUTPUT_DIR, 'reliability_diagram.png'))\n",
        "        plt.show()\n",
        "\n",
        "        return ece.item()\n",
        "\n",
        "print(\"✅ Advanced techniques functions defined.\")\n"
      ],
      "metadata": {
        "id": "Q0pqtCUsB9oV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f97fc9-658a-44e9-c27d-bb1f1896117a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Advanced techniques functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple ANSI color helper for pretty console output\n",
        "class C:\n",
        "    HEADER = \"\\033[95m\"\n",
        "    BLUE = \"\\033[94m\"\n",
        "    CYAN = \"\\033[96m\"\n",
        "    GREEN = \"\\033[92m\"\n",
        "    YELLOW = \"\\033[93m\"\n",
        "    RED = \"\\033[91m\"\n",
        "    BOLD = \"\\033[1m\"\n",
        "    DIM = \"\\033[2m\"\n",
        "    RESET = \"\\033[0m\"\n"
      ],
      "metadata": {
        "id": "Vu18zehHve5f"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 11: MAIN EXECUTION PIPELINE"
      ],
      "metadata": {
        "id": "D9s-hG7CB_8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The main function that orchestrates the entire training and evaluation process.\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "    print(f\"{C.CYAN}{C.BOLD}🚀 Starting Bone Fracture Detection Pipeline{C.RESET}\")\n",
        "\n",
        "    # Phase 1: Data Preparation\n",
        "    print(f\"\\n{C.HEADER}{C.BOLD}=== Phase 1: Data Preparation ==={C.RESET}\")\n",
        "    data_path = DatasetManager.load_dataset(CFG.DATA_PATH)\n",
        "    image_datasets = DatasetManager.create_dataset_splits(data_path, CFG.IMAGE_SIZE)\n",
        "    dataloaders, dataset_sizes, class_names = DatasetManager.create_dataloaders(\n",
        "        image_datasets, CFG.BATCH_SIZE\n",
        "    )\n",
        "    metadata = DatasetManager.prepare_metadata(image_datasets)\n",
        "    print(f\"{C.GREEN}✅ Data prepared. Splits -> \"\n",
        "          f\"Train: {dataset_sizes['train']}, \"\n",
        "          f\"Val: {dataset_sizes['val']}, \"\n",
        "          f\"Test: {dataset_sizes['test']}{C.RESET}\")\n",
        "    print(f\"{C.DIM}Classes: {class_names}{C.RESET}\")\n",
        "\n",
        "    # Phase 2: Model Construction\n",
        "    print(f\"\\n{C.HEADER}{C.BOLD}=== Phase 2: Model Construction ==={C.RESET}\")\n",
        "    model = HybridModel(\n",
        "        cnn_model_name=CFG.MODEL_NAME,\n",
        "        num_classes=CFG.NUM_CLASSES,\n",
        "        d_model=CFG.D_MODEL,\n",
        "        nhead=CFG.NHEAD,\n",
        "        num_encoder_layers=CFG.NUM_ENCODER_LAYERS,\n",
        "        dim_feedforward=CFG.DIM_FEEDFORWARD,\n",
        "        dropout=CFG.DROPOUT\n",
        "    ).to(CFG.device)\n",
        "    ModelUtils.initialize_weights(model)\n",
        "    ModelUtils.freeze_backbone(model)\n",
        "    print(f\"{C.GREEN}✅ Model initialized on device: {CFG.device}{C.RESET}\")\n",
        "    print(f\"{C.DIM}Backbone: {CFG.MODEL_NAME}, Transformer d_model={CFG.D_MODEL}{C.RESET}\")\n",
        "\n",
        "    # Phase 3: Training Pipeline\n",
        "    print(f\"\\n{C.HEADER}{C.BOLD}=== Phase 3: Training Pipeline ==={C.RESET}\")\n",
        "    criterion = ModelUtils.get_loss_function()\n",
        "    optimizer = ModelUtils.get_optimizer(model, CFG.LEARNING_RATE)\n",
        "    scheduler = ModelUtils.get_lr_scheduler(optimizer, epochs=CFG.EPOCHS)\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    # For patience-based early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(CFG.EPOCHS):\n",
        "        print(\n",
        "            f\"\\n{C.BLUE}{C.BOLD}Epoch {epoch + 1}/{CFG.EPOCHS}\"\n",
        "            f\"{C.RESET}\\n{C.DIM}\" + \"-\" * 40 + f\"{C.RESET}\"\n",
        "        )\n",
        "\n",
        "        # Unfreeze backbone halfway through training and reduce LR\n",
        "        if epoch == CFG.EPOCHS // 2:\n",
        "            print(f\"{C.YELLOW}🔓 Unfreezing backbone and lowering learning rate\"\n",
        "                  f\" to {CFG.LEARNING_RATE * 0.1:.1e}{C.RESET}\")\n",
        "            ModelUtils.unfreeze_backbone(model)\n",
        "            optimizer = ModelUtils.get_optimizer(model, CFG.LEARNING_RATE * 0.1)\n",
        "            scheduler = ModelUtils.get_lr_scheduler(\n",
        "                optimizer, epochs=CFG.EPOCHS - epoch\n",
        "            )\n",
        "\n",
        "        train_loss, train_acc = TrainingUtils.train_one_epoch(\n",
        "            model, dataloaders['train'], criterion, optimizer, CFG.device\n",
        "        )\n",
        "        val_loss, val_acc, _, _, _ = TrainingUtils.evaluate(\n",
        "            model, dataloaders['val'], criterion, CFG.device\n",
        "        )\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(\n",
        "            f\"{C.CYAN}📊 Epoch summary -> \"\n",
        "            f\"Train Loss: {train_loss:.4f}, \"\n",
        "            f\"Train Acc: {train_acc:.4f}, \"\n",
        "            f\"Val Loss: {val_loss:.4f}, \"\n",
        "            f\"Val Acc: {val_acc:.4f}{C.RESET}\"\n",
        "        )\n",
        "\n",
        "        # Save best model by validation accuracy\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            ModelUtils.save_checkpoint(\n",
        "                model,\n",
        "                optimizer,\n",
        "                epoch,\n",
        "                val_acc,\n",
        "                os.path.join(CFG.OUTPUT_DIR, 'best_model.pth')\n",
        "            )\n",
        "            print(\n",
        "                f\"{C.GREEN}💾 New best model saved with \"\n",
        "                f\"Val Acc: {val_acc:.4f}{C.RESET}\"\n",
        "            )\n",
        "\n",
        "        # Patience-based early stopping on validation loss\n",
        "        if val_loss < best_val_loss - 1e-4:  # small delta to avoid noise\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(\n",
        "                f\"{C.YELLOW}⏳ Early stopping patience: \"\n",
        "                f\"{patience_counter}/{CFG.EARLY_STOPPING_PATIENCE}{C.RESET}\"\n",
        "            )\n",
        "            if patience_counter >= CFG.EARLY_STOPPING_PATIENCE:\n",
        "                print(\n",
        "                    f\"{C.RED}{C.BOLD}🛑 Early stopping triggered \"\n",
        "                    f\"(no improvement in validation loss).{C.RESET}\"\n",
        "                )\n",
        "                break\n",
        "\n",
        "    # Phase 4: Model Evaluation\n",
        "    print(f\"\\n{C.HEADER}{C.BOLD}=== Phase 4: Model Evaluation ==={C.RESET}\")\n",
        "    best_model = HybridModel(\n",
        "        cnn_model_name=CFG.MODEL_NAME,\n",
        "        num_classes=CFG.NUM_CLASSES,\n",
        "        d_model=CFG.D_MODEL,\n",
        "        nhead=CFG.NHEAD,\n",
        "        num_encoder_layers=CFG.NUM_ENCODER_LAYERS,\n",
        "        dim_feedforward=CFG.DIM_FEEDFORWARD,\n",
        "        dropout=CFG.DROPOUT\n",
        "    ).to(CFG.device)\n",
        "    best_model = ModelUtils.load_best_model(\n",
        "        best_model,\n",
        "        os.path.join(CFG.OUTPUT_DIR, 'best_model.pth')\n",
        "    )\n",
        "\n",
        "    test_loss, test_acc, test_labels, test_preds, test_probs = TrainingUtils.test_inference(\n",
        "        best_model, dataloaders['test'], criterion, CFG.device\n",
        "    )\n",
        "    test_metrics = MetricsAndVisualization.compute_metrics(\n",
        "        test_labels, test_preds, test_probs\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{C.CYAN}{C.BOLD}📈 Test set performance:{C.RESET}\")\n",
        "    print(\n",
        "        f\"{C.GREEN}\"\n",
        "        f\"  • Accuracy : {test_metrics['accuracy']:.4f}\\n\"\n",
        "        f\"  • Precision: {test_metrics['precision']:.4f}\\n\"\n",
        "        f\"  • Recall   : {test_metrics['recall']:.4f}\\n\"\n",
        "        f\"  • F1-score : {test_metrics['f1']:.4f}\\n\"\n",
        "        f\"  • AUC      : {test_metrics['auc']:.4f}\"\n",
        "        f\"{C.RESET}\"\n",
        "    )\n",
        "\n",
        "    MetricsAndVisualization.plot_curves(history, test_acc, test_metrics['auc'])\n",
        "    MetricsAndVisualization.plot_confusion_matrix(test_labels, test_preds, class_names)\n",
        "    MetricsAndVisualization.analyze_predictions(\n",
        "        best_model, dataloaders['test'], class_names, CFG.device\n",
        "    )\n",
        "\n",
        "    # Phase 4.5: Grad-CAM Visualization\n",
        "    print(f\"\\n{C.HEADER}{C.BOLD}=== Phase 4.5: Grad-CAM Visualization ==={C.RESET}\")\n",
        "    GradCAMUtils.generate_and_save_gradcam_samples(\n",
        "        best_model,\n",
        "        dataloaders['test'],\n",
        "        class_names,\n",
        "        CFG.device,\n",
        "        num_samples=5\n",
        "    )\n",
        "    print(f\"{C.GREEN}✅ Grad-CAM samples generated and saved.{C.RESET}\")\n",
        "\n",
        "    # Phase 5: Robustness & Optimization\n",
        "    print(f\"\\n{C.HEADER}{C.BOLD}=== Phase 5: Robustness & Optimization ==={C.RESET}\")\n",
        "    print(f\"{C.CYAN}Starting K-Fold Cross-Validation...{C.RESET}\")\n",
        "    k_fold_results = AdvancedTechniques.k_fold_cross_validation(\n",
        "        CFG.DATA_PATH, CFG.IMAGE_SIZE, CFG.BATCH_SIZE\n",
        "    )\n",
        "    AdvancedTechniques.plot_kfold_results(k_fold_results)\n",
        "\n",
        "    print(f\"\\n{C.CYAN}Starting hyperparameter tuning...{C.RESET}\")\n",
        "    best_params = AdvancedTechniques.hyperparameter_tuning(\n",
        "        dataloaders['train'], dataloaders['val']\n",
        "    )\n",
        "    print(f\"{C.GREEN}✅ Best hyperparameters found: {best_params}{C.RESET}\")\n",
        "\n",
        "    ece = AdvancedTechniques.evaluate_model_calibration(\n",
        "        best_model, dataloaders['test'], CFG.device\n",
        "    )\n",
        "    print(f\"{C.CYAN}Calibration ECE: {ece:.4f}{C.RESET}\")\n",
        "\n",
        "    print(f\"\\n{C.GREEN}{C.BOLD}🎉 Pipeline completed successfully!{C.RESET}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Z0Q78ZkGCBhO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "be0fd30ed3684c1182094f65215f9b9e",
            "6fe287b079aa4b028d27361d81740e56",
            "c84d1b36c9224a0f951570f9bed5eb5d",
            "aefef68bdee841c1b3c42e907da37e7a",
            "3e10cf197f754b23b6f4c24668c99925",
            "2ec9a41636f44a41a0e145284365a450",
            "69833ff04fa74eaebf9fc9c2921acb4e",
            "eb7aa42d4661434f942f0fc35a08eba2",
            "72968c0abd5942049e889f931da11cdb",
            "e5f47b20d2f84c308a657629e0483f30",
            "8f41aab06375459f9d316d2148cb0e44"
          ]
        },
        "outputId": "2aecad96-47e6-4d7b-ecde-d2170d8260e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m\u001b[1m🚀 Starting Bone Fracture Detection Pipeline\u001b[0m\n",
            "\n",
            "\u001b[95m\u001b[1m=== Phase 1: Data Preparation ===\u001b[0m\n",
            "Loading dataset from /content/drive/MyDrive/SaucyDataset\n",
            "Datasets loaded. Class names: ['fracture', 'normal']\n",
            "   - Train set size: 1701 images\n",
            "   - Val set size: 212 images\n",
            "   - Test set size: 214 images\n",
            "Metadata prepared and saved to /content/drive/MyDrive/SaucyDataset_Output/metadata.json\n",
            "\u001b[92m✅ Data prepared. Splits -> Train: 1701, Val: 212, Test: 214\u001b[0m\n",
            "\u001b[2mClasses: ['fracture', 'normal']\u001b[0m\n",
            "\n",
            "\u001b[95m\u001b[1m=== Phase 2: Model Construction ===\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/86.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be0fd30ed3684c1182094f65215f9b9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights initialized\n",
            "Backbone frozen\n",
            "\u001b[92m✅ Model initialized on device: cuda:0\u001b[0m\n",
            "\u001b[2mBackbone: tf_efficientnetv2_s.in1k, Transformer d_model=256\u001b[0m\n",
            "\n",
            "\u001b[95m\u001b[1m=== Phase 3: Training Pipeline ===\u001b[0m\n",
            "\n",
            "\u001b[94m\u001b[1mEpoch 1/30\u001b[0m\n",
            "\u001b[2m----------------------------------------\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 213/213 [01:03<00:00,  3.33batch/s, acc=0.9353, loss=0.875]\n",
            "Validation: 100%|██████████| 27/27 [00:27<00:00,  1.03s/batch]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m📊 Epoch summary -> Train Loss: 0.2625, Train Acc: 0.9353, Val Loss: 0.2289, Val Acc: 0.9434\u001b[0m\n",
            "Checkpoint saved to /content/drive/MyDrive/SaucyDataset_Output/best_model.pth\n",
            "\u001b[92m💾 New best model saved with Val Acc: 0.9434\u001b[0m\n",
            "\n",
            "\u001b[94m\u001b[1mEpoch 2/30\u001b[0m\n",
            "\u001b[2m----------------------------------------\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 213/213 [00:29<00:00,  7.19batch/s, acc=0.9377, loss=0.0483]\n",
            "Validation: 100%|██████████| 27/27 [00:02<00:00, 12.10batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m📊 Epoch summary -> Train Loss: 0.1882, Train Acc: 0.9377, Val Loss: 0.1814, Val Acc: 0.9434\u001b[0m\n",
            "\n",
            "\u001b[94m\u001b[1mEpoch 3/30\u001b[0m\n",
            "\u001b[2m----------------------------------------\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 213/213 [00:27<00:00,  7.68batch/s, acc=0.9506, loss=0.0376]\n",
            "Validation: 100%|██████████| 27/27 [00:02<00:00, 11.98batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m📊 Epoch summary -> Train Loss: 0.1455, Train Acc: 0.9506, Val Loss: 0.1473, Val Acc: 0.9434\u001b[0m\n",
            "\n",
            "\u001b[94m\u001b[1mEpoch 4/30\u001b[0m\n",
            "\u001b[2m----------------------------------------\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 213/213 [00:30<00:00,  6.93batch/s, acc=0.9583, loss=0.0015]\n",
            "Validation: 100%|██████████| 27/27 [00:02<00:00, 11.48batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m📊 Epoch summary -> Train Loss: 0.1267, Train Acc: 0.9583, Val Loss: 0.1699, Val Acc: 0.9481\u001b[0m\n",
            "Checkpoint saved to /content/drive/MyDrive/SaucyDataset_Output/best_model.pth\n",
            "\u001b[92m💾 New best model saved with Val Acc: 0.9481\u001b[0m\n",
            "\u001b[93m⏳ Early stopping patience: 1/5\u001b[0m\n",
            "\n",
            "\u001b[94m\u001b[1mEpoch 5/30\u001b[0m\n",
            "\u001b[2m----------------------------------------\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 213/213 [00:31<00:00,  6.86batch/s, acc=0.9500, loss=0.344]\n",
            "Validation: 100%|██████████| 27/27 [00:02<00:00, 10.43batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m📊 Epoch summary -> Train Loss: 0.1245, Train Acc: 0.9500, Val Loss: 0.1347, Val Acc: 0.9575\u001b[0m\n",
            "Checkpoint saved to /content/drive/MyDrive/SaucyDataset_Output/best_model.pth\n",
            "\u001b[92m💾 New best model saved with Val Acc: 0.9575\u001b[0m\n",
            "\n",
            "\u001b[94m\u001b[1mEpoch 6/30\u001b[0m\n",
            "\u001b[2m----------------------------------------\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 213/213 [00:29<00:00,  7.15batch/s, acc=0.9600, loss=0.0122]\n",
            "Validation: 100%|██████████| 27/27 [00:02<00:00, 12.04batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m📊 Epoch summary -> Train Loss: 0.1035, Train Acc: 0.9600, Val Loss: 0.9824, Val Acc: 0.6934\u001b[0m\n",
            "\u001b[93m⏳ Early stopping patience: 1/5\u001b[0m\n",
            "\n",
            "\u001b[94m\u001b[1mEpoch 7/30\u001b[0m\n",
            "\u001b[2m----------------------------------------\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 213/213 [00:27<00:00,  7.69batch/s, acc=0.9636, loss=0.0123]\n",
            "Validation: 100%|██████████| 27/27 [00:02<00:00, 11.93batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m📊 Epoch summary -> Train Loss: 0.1060, Train Acc: 0.9636, Val Loss: 0.2617, Val Acc: 0.8538\u001b[0m\n",
            "\u001b[93m⏳ Early stopping patience: 2/5\u001b[0m\n",
            "\n",
            "\u001b[94m\u001b[1mEpoch 8/30\u001b[0m\n",
            "\u001b[2m----------------------------------------\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 213/213 [00:28<00:00,  7.51batch/s, acc=0.9600, loss=0.0059]\n",
            "Validation: 100%|██████████| 27/27 [00:03<00:00,  8.36batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m📊 Epoch summary -> Train Loss: 0.1088, Train Acc: 0.9600, Val Loss: 0.2681, Val Acc: 0.8679\u001b[0m\n",
            "\u001b[93m⏳ Early stopping patience: 3/5\u001b[0m\n",
            "\n",
            "\u001b[94m\u001b[1mEpoch 9/30\u001b[0m\n",
            "\u001b[2m----------------------------------------\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 213/213 [00:27<00:00,  7.71batch/s, acc=0.9594, loss=0.0401]\n",
            "Validation: 100%|██████████| 27/27 [00:02<00:00,  9.18batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m📊 Epoch summary -> Train Loss: 0.0991, Train Acc: 0.9594, Val Loss: 0.1540, Val Acc: 0.9575\u001b[0m\n",
            "\u001b[93m⏳ Early stopping patience: 4/5\u001b[0m\n",
            "\n",
            "\u001b[94m\u001b[1mEpoch 10/30\u001b[0m\n",
            "\u001b[2m----------------------------------------\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  34%|███▍      | 73/213 [00:10<00:26,  5.25batch/s, acc=0.9692, loss=0.0748]"
          ]
        }
      ]
    }
  ]
}